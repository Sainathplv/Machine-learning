{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IltiGTvaQb-1"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VmBsrtsxGk-c"
   },
   "source": [
    "# HW1: Classification on simple datasets\n",
    "## Goal: \n",
    "1. get familiar with the machine learning processes.\n",
    "2. understand the applications of KNN, SVM, and Logistic Regression.\n",
    "3. try different parameters of the networks, and determine the best one.\n",
    "4. finish the coding\n",
    "\n",
    "## Datasets:\n",
    "Two famous ML datasets:\n",
    "- Iris dataset\n",
    " - The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant(Setosa(0), Versicolor(1),  Virginica(2)). \n",
    " - each data instance contains \n",
    "    1. sepal length in cm\n",
    "    2. sepal width in cm\n",
    "    3. petal length in cm\n",
    "    4. petal width in cm\n",
    "- The Street View House Numbers (SVHN) Dataset\n",
    " - a colorful handwritten digits database collected from house numbers in Google Street View images, containing 73257 digits for training, 26032 digits for testing, and 531131 additional.\n",
    " - each data is a 32x32 color image corresponding to a digit from 0-9.\n",
    "\n",
    "## Method:\n",
    "KNN, SVM, and Logistic Regression\n",
    "\n",
    "## Submission:\n",
    " 1. The final version of this file (rename it to HW1_yourName.ipynb)\n",
    " 2. A simple report (.doc/.docx) that contains the information below\n",
    "  - Results part\n",
    "     - All the results with different parameters (in table format)\n",
    "     - Screenshots of the learning curves\n",
    "  - Discussion part\n",
    "     - Can you find the best parameter?\n",
    "     - Why this parameter is better than the others?\n",
    "\n",
    "\n",
    "Upload these two files to Canvas separately, without compressing them into a zip file\n",
    "\n",
    "## Grading:\n",
    "- Total: 100 points\n",
    "- For each dataset, each method is worth 10 points. (60 points total for both datasets)\n",
    "- For each dataset, the discussion is worth 20 points. (40 points total for both datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IXJCSUDrLB6H"
   },
   "source": [
    "# 1. Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F_oRq0cMTLMb"
   },
   "outputs": [],
   "source": [
    "# step one: import the needed packages\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "97v6ro6NuTS-"
   },
   "outputs": [],
   "source": [
    "iris_raw = datasets.load_iris() # load iris dataset form sklearn library\n",
    "iris_raw.keys() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zYxdDmwHLjAk"
   },
   "source": [
    "## Checking the contents of the data \n",
    "\n",
    "wonder what does the raw data look like? Try to print out the variable 'iris_raw' in an empty code block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g2vh3HibumaE"
   },
   "outputs": [],
   "source": [
    "iris = pd.DataFrame(data = np.c_[iris_raw['data'], iris_raw['target']],\n",
    "                    columns= iris_raw['feature_names']+['target']) # Convert raw data into an easy-to-read format\n",
    "iris.head(10) # check the first 10 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KM-0VNXZNDzS"
   },
   "outputs": [],
   "source": [
    "# Add the name of the species corresponding to the target\n",
    "species = []\n",
    "for i in range(len(iris['target'])):\n",
    "  if iris['target'][i] == 0:\n",
    "    species.append('setona')\n",
    "  elif iris['target'][i] == 1:\n",
    "    species.append('versicolor')\n",
    "  else:\n",
    "    species.append('virginica')\n",
    "iris['species'] = species\n",
    "iris.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1AgrqHwPrOu9"
   },
   "source": [
    "### train/test data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_blPAElJrN0q"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split # import the package for train/test spliting\n",
    "X = iris.drop(['target', 'species'], axis=1) # or X = iris_raw['data']\n",
    "y = iris['target'] \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,  test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RRya96J3NHEK"
   },
   "source": [
    "# Model training and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E0Vo55Po0z5Q"
   },
   "source": [
    "## 1) KNN\n",
    "\n",
    "- Try different parameters of \"n_neighbors\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PEk66PrXz6S2"
   },
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "El3xp2_lOnyL"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier # import the package for KNN\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train, y_train) # training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v9_HiJZm0IIf"
   },
   "source": [
    "### testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zy2OxL-KCkGK"
   },
   "outputs": [],
   "source": [
    "# testing\n",
    "knn_predictions = knn.predict(X_test)\n",
    "knn_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S63RnP5F0KPJ"
   },
   "source": [
    "### printing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xhw5TUzT-MVj"
   },
   "outputs": [],
   "source": [
    "# printing the results\n",
    "from sklearn import metrics\n",
    "print('Precision, Recall, Confusion matrix, intraining\\n')\n",
    "\n",
    "print(metrics.classification_report(y_test, knn_predictions, digits=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1uqOTXyu0Myq"
   },
   "source": [
    "### plotting the learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l1ZgwtlwFuKV"
   },
   "outputs": [],
   "source": [
    "# Use this block as a black box for plotting the learining curve\n",
    "# Feel free to dive into the code and figure out how it works\n",
    "# There's no need to copy and paste this block again.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "\n",
    "def plot_learning_curve(\n",
    "    estimator,\n",
    "    title,\n",
    "    X,\n",
    "    y,\n",
    "    axes=None,\n",
    "    ylim=None,\n",
    "    cv=None,\n",
    "    n_jobs=None,\n",
    "    scoring=None,\n",
    "    train_sizes=np.linspace(0.1, 1.0, 5),\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate 3 plots: the test and training learning curve, the training\n",
    "    samples vs fit times curve, the fit times vs score curve.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : estimator instance\n",
    "        An estimator instance implementing `fit` and `predict` methods which\n",
    "        will be cloned for each validation.\n",
    "\n",
    "    title : str\n",
    "        Title for the chart.\n",
    "\n",
    "    X : array-like of shape (n_samples, n_features)\n",
    "        Training vector, where ``n_samples`` is the number of samples and\n",
    "        ``n_features`` is the number of features.\n",
    "\n",
    "    y : array-like of shape (n_samples) or (n_samples, n_features)\n",
    "        Target relative to ``X`` for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    axes : array-like of shape (3,), default=None\n",
    "        Axes to use for plotting the curves.\n",
    "\n",
    "    ylim : tuple of shape (2,), default=None\n",
    "        Defines minimum and maximum y-values plotted, e.g. (ymin, ymax).\n",
    "\n",
    "    cv : int, cross-validation generator or an iterable, default=None\n",
    "        Determines the cross-validation splitting strategy.\n",
    "        Possible inputs for cv are:\n",
    "\n",
    "          - None, to use the default 5-fold cross-validation,\n",
    "          - integer, to specify the number of folds.\n",
    "          - :term:`CV splitter`,\n",
    "          - An iterable yielding (train, test) splits as arrays of indices.\n",
    "\n",
    "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
    "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
    "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
    "\n",
    "        Refer :ref:`User Guide <cross_validation>` for the various\n",
    "        cross-validators that can be used here.\n",
    "\n",
    "    n_jobs : int or None, default=None\n",
    "        Number of jobs to run in parallel.\n",
    "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
    "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
    "        for more details.\n",
    "\n",
    "    scoring : str or callable, default=None\n",
    "        A str (see model evaluation documentation) or\n",
    "        a scorer callable object / function with signature\n",
    "        ``scorer(estimator, X, y)``.\n",
    "\n",
    "    train_sizes : array-like of shape (n_ticks,)\n",
    "        Relative or absolute numbers of training examples that will be used to\n",
    "        generate the learning curve. If the ``dtype`` is float, it is regarded\n",
    "        as a fraction of the maximum size of the training set (that is\n",
    "        determined by the selected validation method), i.e. it has to be within\n",
    "        (0, 1]. Otherwise it is interpreted as absolute sizes of the training\n",
    "        sets. Note that for classification the number of samples usually have\n",
    "        to be big enough to contain at least one sample from each class.\n",
    "        (default: np.linspace(0.1, 1.0, 5))\n",
    "    \"\"\"\n",
    "\n",
    "    axes.set_title(title)\n",
    "    if ylim is not None:\n",
    "        axes.set_ylim(*ylim)\n",
    "    axes.set_xlabel(\"Training examples\")\n",
    "    axes.set_ylabel(\"Score\")\n",
    "\n",
    "    train_sizes, train_scores, test_scores, fit_times, _ = learning_curve(\n",
    "        estimator,\n",
    "        X,\n",
    "        y,\n",
    "        scoring=scoring,\n",
    "        cv=cv,\n",
    "        n_jobs=n_jobs,\n",
    "        train_sizes=train_sizes,\n",
    "        return_times=True,\n",
    "    )\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    fit_times_mean = np.mean(fit_times, axis=1)\n",
    "    fit_times_std = np.std(fit_times, axis=1)\n",
    "\n",
    "    # Plot learning curve\n",
    "    axes.grid()\n",
    "    axes.fill_between(\n",
    "        train_sizes,\n",
    "        train_scores_mean - train_scores_std,\n",
    "        train_scores_mean + train_scores_std,\n",
    "        alpha=0.1,\n",
    "        color=\"r\",\n",
    "    )\n",
    "    axes.fill_between(\n",
    "        train_sizes,\n",
    "        test_scores_mean - test_scores_std,\n",
    "        test_scores_mean + test_scores_std,\n",
    "        alpha=0.1,\n",
    "        color=\"g\",\n",
    "    )\n",
    "    axes.plot(\n",
    "        train_sizes, train_scores_mean, \"o-\", color=\"r\", label=\"Training score\"\n",
    "    )\n",
    "    axes.plot(\n",
    "        train_sizes, test_scores_mean, \"o-\", color=\"g\", label=\"Cross-validation score\"\n",
    "    )\n",
    "    axes.legend(loc=\"best\")\n",
    "    return plt\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tNxSwsKGUv6O"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots()\n",
    "cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n",
    "plot_learning_curve( knn, 'training curve for KNN', X, y, axes=axes, ylim=(0.7, 1.01), cv = cv, n_jobs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lNIHUxA-UvGr"
   },
   "source": [
    "## 2) SVM\n",
    "\n",
    "- Try different kernels, and record the results\n",
    "- For more information, refer to https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "- finish the code\n",
    "- try different parameters of \"kernel\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VLXIdXo-uHmE"
   },
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mxe1pJFb0_Fq"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf = SVC(kernel='rbf')\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UvnCDDScuNAK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5LC3Bdbu0_wX"
   },
   "source": [
    "## 3) Logistic regression\n",
    "- Try different regularization methods\n",
    "- Please refer to https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html \n",
    "- finish the code\n",
    "- try different parameters of \"penalty\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D9NgUaiazhm8"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "log_reg = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "log_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f9B5uS9Y4sP3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BUqy8GY80jHS"
   },
   "source": [
    "# 2. SVHN Dataset\n",
    "- download data from http://ufldl.stanford.edu/housenumbers/ \n",
    "- finish the code, and record the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ud8j3F6rxwHE"
   },
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "# loading data\n",
    "train_data = sio.loadmat('train_32x32.mat')\n",
    "test_data = sio.loadmat('test_32x32.mat')\n",
    "\n",
    "# training set\n",
    "length = 5000 # We use the first 5000 instances here for simplicity. But feel free to use more instances if you don't mind long training sessions.\n",
    "X_train = np.zeros([length,1024])\n",
    "y_train = np.zeros([length,1])\n",
    "for i in range(length):\n",
    "    data = np.mean(train_data['X'][:,:,:,i], axis=2) # transform data to be compatible for training \n",
    "    X_train[i] = data.flatten()\n",
    "    y_train[i] = train_data['y'][i]\n",
    "  \n",
    "# testing set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hYopLVRj2jgy"
   },
   "source": [
    "Checking the contents of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uA2WGney10IR"
   },
   "outputs": [],
   "source": [
    "# show sample\n",
    "# try to run this block multiple times\n",
    "import random\n",
    "image_ind = random.randint(0,5000)\n",
    "plt.imshow(train_data['X'][:,:,:,image_ind])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "y_train[image_ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wlP3Y-xj21U5"
   },
   "source": [
    "## 1) KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ASDerH6T23r-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i0mWvqmt24n0"
   },
   "source": [
    "## 2) SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ILeSfkf926QU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sn_QCUQE263m"
   },
   "source": [
    "## 3) Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tFRwVfHV4s6E"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
